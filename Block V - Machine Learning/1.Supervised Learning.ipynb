{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **1. Supervised Learning**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In Supervised Learning the predicted values are known. The aim is to predict the target values of unseen data, given the features.\n","\n","**Types of supervised Learning:**\n","\n","- Classification: Target variable consists of categories\n","- Regression: Target variable is continous.\n","\n","**Feature** = predictor variable = independent variable \n","\n","**Target variable** = dependent variable = response variable\n","\n","\n","**Requirements** before using Supervised Learning\n","- No missing values\n","- Data in numeric format\n","- Data stored in pandas DataFrames or NumPy Arrays.\n","\n","Perform Exploratory Data Analysis (EDA) first.\n","\n","\n","**Scikit-Learn Syntax:**\n","- from sklearn.module import Model\n","- model = Model()\n","- model.fit(X,y)\n","- predictions = model.predict(X_new)\n","- print(predictions)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Classification Problem**\n","\n","Let $\\mathcal{X}$ be the feature space, and $\\mathcal{Y}$ be the label space. We are given a dataset $D = {(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)}$, where $\\mathbf{x}_i \\in \\mathcal{X}$ represents a feature vector and $y_i \\in \\mathcal{Y}$ represents the corresponding class label.\n","\n","The goal of a classification problem is to learn a function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that maps an input feature vector $\\mathbf{x} \\in \\mathcal{X}$ to its corresponding class label $y \\in \\mathcal{Y}$. The function $f$ represents the classifier.\n","\n","\n","#### **Regression Problem:**\n","Let $\\mathcal{X}$ be the feature space, and $\\mathcal{Y}$ be the target space. We are given a dataset $D = {(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)}$, where $\\mathbf{x}_i \\in \\mathcal{X}$ represents a feature vector and $y_i \\in \\mathcal{Y}$ represents the corresponding target value.\n","\n","The goal of a regression problem is to learn a function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that maps an input feature vector $\\mathbf{x} \\in \\mathcal{X}$ to its corresponding target value $y \\in \\mathcal{Y}$. The function $f$ represents the regression model.\n","\n","\n","**In both cases**, the learning process involves finding the optimal parameters or coefficients for the function $f$, typically denoted as $\\theta$ (or $\\beta$), by minimizing a suitable loss or error function. The choice of the loss function depends on the specific algorithm and problem at hand."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **1.1. Classification**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","churn_df = pd.read_csv('../datasets/telecom_churn_clean.csv', index_col=0)\n","churn_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **The k-Nearest Neighbors (kNN) Algorithm**\n","\n","The k-Nearest Neighbors (k-NN) algorithm is a non-parametric classification and regression method. \n","\n","The goal of the KNN algorithm is to predict the label of a new input vector $x$ by finding the $k$ nearest neighbors in the training set and assigning the label based on a majority vote.\n","\n","Let $d(x, x_i)$ be a distance metric that measures the distance between the new input vector $x$ and a training vector $x_i$. The most commonly used distance metric is the Euclidean distance:\n","\n","$d(x, x_i) = \\sqrt{\\sum_{j=1}^{m}(x_{ij} - x_{ij})^2}$\n","\n","where $m$ is the number of features in each vector.\n","\n","To predict the label of $x$, the KNN algorithm follows these steps:\n","\n","1. Compute the distance $d(x, x_i)$ between the new input vector $x$ and each training vector $x_i$.\n","2. Select the $k$ training vectors with the smallest distances.\n","3. Count the number of occurrences of each label among the selected $k$ training vectors.\n","4. Assign the label with the highest count as the predicted label for $x$.\n","\n","The KNN algorithm is a non-parametric and lazy learning algorithm, meaning it does not make any assumptions about the underlying data distribution and postpones the generalization process until a new input is encountered.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **kNN from Scratch:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","class KNN:\n","    def __init__(self, k):\n","        self.k = k\n","        \n","    def fit(self, X, y):\n","        self.X_train = X\n","        self.y_train = y\n","    \n","    def euclidean_distance(self, x1, x2):\n","        return np.sqrt(np.sum((x1 - x2) ** 2))\n","    \n","    def predict(self, X):\n","        y_pred = []\n","        \n","        for x in X:\n","            distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n","            indices = np.argsort(distances)[:self.k]\n","            k_nearest_labels = [self.y_train[i] for i in indices]\n","            label_counts = np.bincount([self.label_to_int[label] for label in k_nearest_labels])\n","            most_common = np.argmax(label_counts)\n","            y_pred.append(self.int_to_label[most_common])\n","        \n","        return np.array(y_pred)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Example:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the data points and their corresponding classes\n","X_train = np.array([[2, 3], [4, 2], [1, 3], [3, 6], [7, 2]])\n","y_train = np.array(['Class A', 'Class A', 'Class B', 'Class B', 'Class A'])\n","\n","k = 3  # Specify the number of nearest neighbors to consider\n","knn = KNN(k)\n","knn.fit(X_train, y_train)\n","\n","X_new = np.array([[5, 4]])\n","\n","# Map class labels to integers\n","labels = np.unique(y_train)\n","knn.label_to_int = {label: i for i, label in enumerate(labels)}\n","knn.int_to_label = {i: label for i, label in enumerate(labels)}\n","\n","y_pred = knn.predict(X_new)\n","print(\"Predicted class:\", y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert class labels to numeric values\n","y_train_numeric = np.array([knn.label_to_int[label] for label in y_train])\n","y_pred_numeric = np.array([knn.label_to_int[label] for label in y_pred])\n","\n","# Plotting the data points and predicted class\n","plt.figure()\n","\n","# Plotting the training data points\n","for label in labels:\n","    indices = np.where(y_train == label)\n","    plt.scatter(X_train[indices, 0], X_train[indices, 1], label=label)\n","\n","# Plotting the new data point\n","plt.scatter(X_new[:, 0], X_new[:, 1], c=y_pred_numeric, cmap='Set1', marker='^', label='New Data')\n","\n","plt.title('KNN Classification')\n","plt.xlabel('X')\n","plt.ylabel('Y')\n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **kNN with scikit-learn:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import KNeighborsClassifier\n","from sklearn.neighbors import KNeighborsClassifier \n","\n","\n","# Create arrays for the features and the target variable\n","y = churn_df[\"churn\"].values\n","X = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n","\n","# Create a KNN classifier with 6 neighbors\n","knn = KNeighborsClassifier(n_neighbors=2)\n","\n","# Fit the classifier to the data\n","knn.fit(X, y)\n","\n","X_new = np.array([[30.0, 17.5],\n","                  [107.0, 24.1],\n","                  [213.0, 10.9]])\n","\n","# Predict the labels for the X_new\n","y_pred = knn.predict(X_new)\n","\n","# Print the predictions for X_new\n","print(\"Predictions: {}\".format(y_pred)) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Measuring model performance**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In classification, accuracy is a commonly used metric.\n","\n","\\begin{align*}\n","Accuracy = \\frac{\\text{correct predictions}}{\\text{total observations}}\n","\\end{align*}\n","\n","But it is NOT indicative of how well it can generalize to unseen data, which is what we are interested in!\n","\n","Therefore we **split the data into training set and test set**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = churn_df.drop(\"churn\", axis=1).values\n","y = churn_df[\"churn\"].values\n","\n","# Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","knn = KNeighborsClassifier(n_neighbors=5)\n","\n","# Fit the classifier to the training data\n","knn.fit(X_train, y_train)\n","\n","# Print the accuracy\n","print(knn.score(X_test, y_test))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model complexity and Over/Underfitting**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create neighbors\n","neighbors = np.arange(1, 13)\n","train_accuracies = {}\n","test_accuracies = {}\n","\n","for neighbor in neighbors:\n","    knn = KNeighborsClassifier(n_neighbors=neighbor)\n","    knn.fit(X_train, y_train)\n","    train_accuracies[neighbor] = knn.score(X_train, y_train)\n","    test_accuracies[neighbor] = knn.score(X_test, y_test)\n","\n","\n","# Add a title\n","plt.title(\"KNN: Varying Number of Neighbors\")\n","\n","# Plot training accuracies\n","plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n","\n","# Plot test accuracies\n","plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n","\n","plt.legend()\n","plt.xlabel(\"Number of Neighbors\")\n","plt.ylabel(\"Accuracy\")\n","\n","# Display the plot\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **1.2. Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diabetes_df = pd.read_csv('../datasets/diabetes_clean.csv')\n","diabetes_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Ordinary Least Squares Approach:**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\\begin{align*}\n","y = \\beta_0 + \\beta_1 x + \\epsilon\n","\\end{align*}\n","\n","Simple Linear Regression uses one feature\n","- $y$ = target\n","- $x$ = single feature\n","- $\\beta_0$ and $\\beta_1$ are the coefficients (slope and intercept)\n","- $\\epsilon$ = error\n","\n","\n","**How we choose $\\beta_0$ and $\\beta_1$?**\n","\n","**Choose the line that minimize the error function/loss function**\n","\n","- We want the line to be as close to observations as possible - **best fit**.\n","- Therefore we want to minimize the vertical distances between the fit and the data.\n","- So for each observation, we caluclate the vertical distance betweet it and the line. This distance is called **residual**.\n","- By adding all the squared residuals, we calculate the the residual sum of squares:\n","\n","\\begin{align*}\n","RSS &= \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\\\\n","    &= (y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1))^2 + (y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_2))^2  + \\dots + (y_n - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_n))^2\n","\n","\\end{align*}\n","\n","This type of linar regression is called **Ordinary Least Squares**, where **we minimize the RSS**, so the parameters determined by:\n","\n","\n","\\begin{align*}\n","\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\\n","\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n","\\end{align*}\n","\n","The multiple linear Regression takes the form:\n","\n","\\begin{align*}\n","Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon\n","\\end{align*}\n","\n","\n","The RSS for the multiple linear regression is given by:\n","\n","\\begin{align*}\n","RSS &= \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\\\\n","    &= (y_i  - (\\hat{\\beta}_0 + \\sum_{j=1}^p \\hat{\\beta}_j x_{ij})^2) \\\\\n","    &= (y-X\\hat{\\beta})^T (y-X\\hat{\\beta})\n","\\end{align*}\n","\n","The coefficients that minimizes above RSS are given by:\n","\n","\\begin{align*}\n","\\hat{{\\beta}} = (X^TX)^{-1} X^Ty.\n","\\end{align*}\n","\n","This is the closed-form solution for the parameters $\\hat{\\beta}$​ in the context of ordinary least squares (OLS) linear regression. This method provides the exact solution without the need for iterative optimization. This equation calculates the optimal values for the regression coefficients $\\hat{\\beta}$​ without the need for iterative optimization algorithms like **gradient descent**."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Gradient descent**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The mathematical approach for gradient descent in the context of linear regression involves the following steps:\n","\n","**1. Define the hypothesis function:**\n","\n","\\begin{align*}\n","h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n\n","\\end{align*}\n","\n","**2. Define the cost function:**\n","\n","The cost function measures the error between the predicted values and the actual values. In linear regression, the commonly used cost function is the mean squared error (MSE):\n","\n","\\begin{align*}\n","J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2\n","\\end{align*}\n","\n","**3. Update the parameters using gradient descent:**\n","\n","The goal of gradient descent is to find the optimal parameters that minimize the cost function. The parameters are iteratively updated using the gradients of the cost function with respect to each parameter.\n","\n","The update rule for each parameter $\\theta_j$ is given by:\n","\n","\\begin{align*}\n","\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n","\\end{align*}\n","\n","The partial derivative $\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$ is calculated as:\n","\n","\\begin{align*}\n","\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}\n","\\end{align*}\n","\n","**4. Repeat the parameter updates until convergence:**\n","\n","The parameter updates are repeated iteratively until convergence or a maximum number of iterations is reached. Convergence is typically determined by monitoring the change in the cost function or the parameters.\n","\n","By updating the parameters in the opposite direction of the gradients, gradient descent gradually minimizes the cost function and finds the optimal parameters that best fit the training data.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Linear Regression with scikit-learn**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Simple Linear Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_bmi = diabetes_df['bmi'].values\n","y = diabetes_df['glucose']\n","\n","plt.scatter(X_bmi,y)\n","plt.xlabel('Body Mass Index')\n","plt.ylabel('Blood Glucose (mg/dl)')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_bmi = X_bmi.reshape(-1,1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reg = LinearRegression()\n","reg.fit(X_bmi,y)\n","\n","predictions = reg.predict(X_bmi)\n","\n","plt.scatter(X_bmi,y)\n","plt.plot(X_bmi, predictions, color = 'red')\n","plt.xlabel('Body Mass Index')\n","plt.ylabel('Blood Glucose (mg/dl)')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Multiple Linear Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = diabetes_df.drop('glucose',axis=1).values\n","y = diabetes_df['glucose'].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","reg_all = LinearRegression()\n","reg_all.fit(X_train,y_train)\n","\n","y_pred = reg_all.predict(X_test)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Metric for Linear Regression**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The **default metric** for linear Regression is **$R^2$** (Coefficient of determination), which quantifies the amount of variance in the target variable that is explained by the features. The values for $R^2$ can range from $0$ to $1$, where $1$ meaning the features completely explain the target's variance.\n","\n","Another way to asses a regression model's performance is to take the **mean of the residual sum of squares (MSE)**.\n","\n","\\begin{align*}\n","MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","\\end{align*}\n","\n","MSE is measured in units of our target variable, squared. So we need to take the squared root **Root Mean Squared Error**, so we have the same units as the target variable:\n","$RMSE = \\sqrt{MSE}$."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(reg_all.score(X_test,y_test))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here the features only explain about 28% of blood glucose level variance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RMSE = mean_squared_error(y_test,y_pred, squared=False)\n","print(RMSE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The model has an average error for blood glucose levels of around 24 milligrams per deciliter."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LinearRegression:\n","    def __init__(self, learning_rate=0.01, num_iterations=1000):\n","        self.learning_rate = learning_rate\n","        self.num_iterations = num_iterations\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        num_samples, num_features = X.shape\n","\n","        # Initialize weights and bias as zeros\n","        self.weights = np.zeros(num_features)\n","        self.bias = 0\n","\n","        # Gradient descent\n","        for _ in range(self.num_iterations):\n","            # Calculate predictions\n","            y_pred = np.dot(X, self.weights) + self.bias\n","\n","            # Calculate gradients\n","            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n","            db = (1 / num_samples) * np.sum(y_pred - y)\n","\n","            # Update parameters\n","            self.weights -= self.learning_rate * dw\n","            self.bias -= self.learning_rate * db\n","\n","    def predict(self, X):\n","        # Make predictions\n","        y_pred = np.dot(X, self.weights) + self.bias\n","        return y_pred"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Cross-validation**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Our model-performance is dependent on the way that we split up the data.\n","- Not representative of the model's ability to generalize to unseen data.\n","- **Solution: Cross-validation!**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![Bildbeschreibung](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/2880px-K-fold_cross_validation_EN.svg.png){ width=500 }\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, KFold\n","kf = KFold(n_splits=6, shuffle=True, random_state=42)\n","reg = LinearRegression()\n","cv_results = cross_val_score(reg,X,y, cv = kf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Analyzing cross-validation metrics:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the mean\n","print(np.mean(cv_results))\n","\n","# Print the standard deviation\n","print(np.std(cv_results))\n","\n","# Print the 95% confidence interval\n","print(np.quantile(cv_results, [0.025, 0.975]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Regularized Regression**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Regularization in Regression is a technique used to **avoid overfitting**. \n","\n","- Regularization: Penalize large coefficients"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **(i) Ridge Regression**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\\begin{align*}\n","RSS_{ridge} = RSS + \\alpha \\sum_{j=1}^p \\beta_j^2\n","\\end{align*}\n","\n","\n","**Ridge regression is also referred to as L2 Regularization.**\n","\n","- Ridge penalizes large coefficients or negative coefficients\n","- The parameter $\\alpha$ is known as **hyperparameter** - a variable used to optimize model parameters\n","- $\\alpha$ controls the model complexity"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Calculate the $R^2$ score for each iteration of ridge:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","\n","scores = []\n","for alpha in [0.1, 1.0, 10, 100, 1000]:\n","    ridge = Ridge(alpha = alpha)\n","    ridge.fit(X_train,y_train)\n","    y_pred = ridge.predict(X_test)\n","    scores.append(ridge.score(X_test,y_test))\n","print(scores)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **(ii) Lasso Regression**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here we the RSS function plus the absolute value:\n","\n","\n","\\begin{align*}\n","RSS_{lasso} = RSS + \\alpha \\sum_{j=1}^p |\\beta_j|\n","\\end{align*}\n","\n","**Lasso regression is also referred to as L1 Regularization.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","\n","scores = []\n","for alpha in [0.1, 1.0, 10, 20, 50]:\n","    lasso = Lasso(alpha = alpha)\n","    lasso.fit(X_train,y_train)\n","    y_pred = lasso.predict(X_test)\n","    scores.append(lasso.score(X_test,y_test))\n","print(scores)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Lasso Regression can also used for feature selection:**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["L1 Norm (Lasso regularization):\n","- The L1 norm of a vector is the sum of the absolute values of its elements. In the context of regularization, the L1 norm penalty is added to the objective function of a linear regression model. When the L1 norm is minimized, some of the coefficients will be reduced to zero, effectively performing feature selection. This sparsity-inducing property of the L1 norm allows Lasso to select a subset of features while shrinking the coefficients of less important features towards zero.\n","\n","L2 Norm (Ridge regularization):\n","- The L2 norm of a vector is the square root of the sum of the squared values of its elements. In ridge regularization, the L2 norm penalty is added to the objective function. Unlike L1 norm, the L2 norm does not drive coefficients exactly to zero. Instead, it shrinks their values towards zero, but they rarely become zero. Ridge regularization helps to reduce the impact of less important features without eliminating them completely.\n","\n","**In summary, Lasso uses the L1 norm penalty to encourage sparsity, making it suitable for feature selection.**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![Why LASSO can reduce dimension of feature space?](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jd03Hyt2bpEv1r7UijLlpg.png){ width=600 }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit Lasso regression\n","lasso = Lasso(alpha=0.1)\n","lasso_coef = lasso.fit(X, y)\n","\n","# Get coefficient values and corresponding feature names\n","coefficients = lasso_coef.coef_\n","feature_names = diabetes_df.drop(\"glucose\", axis=1).columns\n","\n","# Create a DataFrame with coefficients and feature names\n","coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n","\n","# Sort the DataFrame by coefficient values\n","coefficients_df = coefficients_df.sort_values('Coefficient')\n","\n","plt.bar(coefficients_df['Feature'], coefficients_df['Coefficient'])\n","plt.xlabel('Features')\n","plt.ylabel('Coefficient')\n","plt.title('Lasso Regression Coefficients')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **1.3. Fine-Tuning your Model**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **1.4. Preprocessing and Pipelines**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
